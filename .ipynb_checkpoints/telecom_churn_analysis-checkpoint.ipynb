{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da83c5b1",
   "metadata": {},
   "source": [
    "# Telecom Churn — End-to-End Analysis and Modeling\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "1. EDA & Initial Cleaning\n",
    "2. Feature Engineering\n",
    "3. Preprocessing & Pipeline (ColumnTransformer)\n",
    "4. Train multiple models (LogisticRegression, RandomForest, GradientBoosting, XGBoost)\n",
    "5. Hyperparameter tuning with GridSearchCV\n",
    "6. Handling class imbalance (SMOTE or class_weight)\n",
    "7. Final evaluation and interpretation\n",
    "\n",
    "**Instructions:** Run the notebook cells sequentially. The dataset `telecom_churn.csv` is expected at `/mnt/data/telecom_churn.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: imports and data load\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = '/mnt/data/telecom_churn.csv'\n",
    "print('Data exists?:', os.path.exists(DATA_PATH))\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('\\\\nDataset shape:', df.shape)\n",
    "print('\\\\nColumns:\\\\n', df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07934a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Initial cleaning - TotalCharges conversion and missing values overview\n",
    "print('Original dtype of TotalCharges:', df['TotalCharges'].dtype if 'TotalCharges' in df.columns else 'N/A')\n",
    "\n",
    "# Convert TotalCharges to numeric, coercing errors to NaN\n",
    "if 'TotalCharges' in df.columns:\n",
    "    df['TotalCharges_clean'] = pd.to_numeric(df['TotalCharges'].astype(str).str.strip(), errors='coerce')\n",
    "\n",
    "# Show rows where conversion failed\n",
    "failed_conv = df[df['TotalCharges_clean'].isna() & df['TotalCharges'].notna()]\n",
    "print('Number of rows where TotalCharges could not be converted to numeric (and original not NA):', len(failed_conv))\n",
    "if len(failed_conv) > 0:\n",
    "    display(failed_conv.head(10))\n",
    "\n",
    "# Replace the original TotalCharges with cleaned numeric (and drop old if desired)\n",
    "if 'TotalCharges' in df.columns:\n",
    "    df['TotalCharges'] = df['TotalCharges_clean']\n",
    "    df.drop(columns=['TotalCharges_clean'], inplace=True)\n",
    "\n",
    "# Missing values overview\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "print('\\\\nColumns with missing values and counts:')\n",
    "print(missing)\n",
    "\n",
    "# Percentage missing\n",
    "missing_pct = (df.isna().sum()/len(df)).sort_values(ascending=False)\n",
    "missing_pct = missing_pct[missing_pct > 0]\n",
    "print('\\\\nPercentage missing:')\n",
    "print((missing_pct*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: EDA Visualizations\n",
    "# 1) Distribution of a numerical feature (TotalCharges) for churn vs non-churn\n",
    "if 'Churn' in df.columns and 'TotalCharges' in df.columns:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    churn_yes = df[df['Churn'] == 'Yes']['TotalCharges'].dropna()\n",
    "    churn_no = df[df['Churn'] == 'No']['TotalCharges'].dropna()\n",
    "    plt.hist(churn_no, bins=50, alpha=0.6, label='No Churn')\n",
    "    plt.hist(churn_yes, bins=50, alpha=0.6, label='Churn')\n",
    "    plt.title('Distribution of TotalCharges: Churn vs No Churn')\n",
    "    plt.xlabel('TotalCharges')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Churn or TotalCharges column not found for this plot.')\n",
    "\n",
    "# 2) Relationship between a categorical feature and churn (e.g., PaymentMethod)\n",
    "cat_col = None\n",
    "for c in ['PaymentMethod', 'PaymentMethod_Credit card', 'PaymentMethod_Electronic check', 'MultipleLines']:\n",
    "    if c in df.columns:\n",
    "        cat_col = c\n",
    "        break\n",
    "\n",
    "if cat_col:\n",
    "    ct = pd.crosstab(df[cat_col].fillna('Missing'), df['Churn'])\n",
    "    ct_pct = ct.div(ct.sum(axis=1), axis=0)\n",
    "    print('\\\\nCounts by {} and Churn:\\\\n'.format(cat_col))\n",
    "    display(ct)\n",
    "    print('\\\\nPercentage by {} and Churn (row-wise):\\\\n'.format(cat_col))\n",
    "    display((ct_pct*100).round(2))\n",
    "    ct.plot(kind='bar', stacked=False, figsize=(10,5))\n",
    "    plt.title(f'Counts by {cat_col} and Churn')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No known categorical column (PaymentMethod/MultipleLines) found for this plot.')\n",
    "\n",
    "# 3) Correlation heatmap for numeric features (simple)\n",
    "num_df = df.select_dtypes(include=['int64','float64'])\n",
    "if num_df.shape[1] > 1:\n",
    "    corr = num_df.corr()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(corr, interpolation='none')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.title('Numeric feature correlation matrix (visual)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987aaea0",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Create at least two meaningful features from existing data. Below we implement two candidates and explain reasons:\n",
    "\n",
    "1. **AvgMonthlyCharge** = TotalCharges / (tenure) when tenure available — gives per-month revenue (helps detect customers who pay a lot early or long-term low spenders).  \n",
    "2. **MultiService** — binary feature indicating customers who have multiple services (e.g., both PhoneService and MultipleLines or additional service flags). This often correlates with stickiness or complexity of churn.\n",
    "\n",
    "(If the specific service columns are absent in the dataset, the notebook adapts to available columns.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Feature engineering - create two new features\n",
    "df_fe = df.copy()\n",
    "# AvgMonthlyCharge: guard against division by zero or missing Tenure\n",
    "if 'TotalCharges' in df_fe.columns and 'tenure' in df_fe.columns:\n",
    "    df_fe['AvgMonthlyCharge'] = df_fe['TotalCharges'] / (df_fe['tenure'].replace(0, np.nan))\n",
    "    # if tenure is zero or NaN, fill AvgMonthlyCharge with TotalCharges (one-off)\n",
    "    df_fe['AvgMonthlyCharge'] = df_fe['AvgMonthlyCharge'].fillna(df_fe['TotalCharges'])\n",
    "else:\n",
    "    # fallback: if 'MonthlyCharges' exists, use it directly\n",
    "    if 'MonthlyCharges' in df_fe.columns:\n",
    "        df_fe['AvgMonthlyCharge'] = df_fe['MonthlyCharges']\n",
    "    else:\n",
    "        df_fe['AvgMonthlyCharge'] = np.nan\n",
    "\n",
    "# MultiService: check some common service columns\n",
    "service_cols = [c for c in df_fe.columns if c.lower() in ['phoneservice','multiplelines','internetservice','onlinebackup','techsupport','streamingtv','streamingmovies']]\n",
    "# also include columns that contain 'Service' in name\n",
    "if not service_cols:\n",
    "    service_cols = [c for c in df_fe.columns if 'Service' in c or 'service' in c]\n",
    "\n",
    "if service_cols:\n",
    "    # Define MultiService as count of 'Yes' across known service indicator columns\n",
    "    def count_yes(row):\n",
    "        cnt = 0\n",
    "        for c in service_cols:\n",
    "            val = str(row.get(c)).strip().lower()\n",
    "            if val in ['yes','true','1']:\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "    df_fe['MultiServiceCount'] = df_fe.apply(count_yes, axis=1)\n",
    "    df_fe['MultiService'] = (df_fe['MultiServiceCount'] > 1).astype(int)\n",
    "else:\n",
    "    df_fe['MultiServiceCount'] = 0\n",
    "    df_fe['MultiService'] = 0\n",
    "\n",
    "print('Feature engineering done. New columns added:', [c for c in ['AvgMonthlyCharge','MultiServiceCount','MultiService'] if c in df_fe.columns])\n",
    "df_fe[['AvgMonthlyCharge','MultiServiceCount','MultiService']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc0d16",
   "metadata": {},
   "source": [
    "## Preprocessing & Pipeline\n",
    "\n",
    "Rules implemented:\n",
    "- Numeric features: median imputation + StandardScaler\n",
    "- Categorical features: most frequent imputation + OneHotEncoder (handle_unknown='ignore')\n",
    "- Boolean features: impute constant 'Unknown' then treat as categorical\n",
    "\n",
    "We drop irrelevant columns like customerID if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Build ColumnTransformer preprocessor and a sample pipeline with LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Prepare feature list: pick columns for modeling using provided top features if available\n",
    "provided_top = [\n",
    "    \"IsHeavyDataUser\",\n",
    "    \"PaymentMethod_Credit card\",\n",
    "    \"PaymentMethod_Bank transfer\",\n",
    "    \"IsHighValueCustomer\",\n",
    "    \"MultipleLines\",\n",
    "    \"SeniorCitizen\",\n",
    "    \"PaymentMethod_Electronic check\",\n",
    "    \"PhoneService\",\n",
    "    \"RevenuePerGB\",\n",
    "    \"TotalCharges\"\n",
    "]\n",
    "\n",
    "# Use available columns intersecting with provided_top, else auto-select\n",
    "features = [c for c in provided_top if c in df_fe.columns]\n",
    "if not features:\n",
    "    # use all except target and ID-like columns\n",
    "    exclude = ['Churn','customerID','customerId','CustomerID']\n",
    "    features = [c for c in df_fe.columns if c not in exclude]\n",
    "\n",
    "print('Features used for modeling (sample):', features[:20])\n",
    "\n",
    "X = df_fe[features].copy()\n",
    "# Build target y\n",
    "if 'Churn' in df_fe.columns:\n",
    "    y = df_fe['Churn'].apply(lambda v: 1 if str(v).strip().lower() in ['yes','true','1'] else 0)\n",
    "else:\n",
    "    raise ValueError('Churn column not found in dataset')\n",
    "\n",
    "# Identify dtypes\n",
    "numeric_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "boolean_features = X.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "# Treat boolean as categorical by converting to object (after imputation step in pipeline)\n",
    "print('Numeric features:', numeric_features)\n",
    "print('Categorical features:', categorical_features)\n",
    "print('Boolean features:', boolean_features)\n",
    "\n",
    "# Imputers and transformers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "# For booleans: impute constant 'Unknown' then onehot encode\n",
    "boolean_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features),\n",
    "    ('bool', boolean_transformer, boolean_features)\n",
    "])\n",
    "\n",
    "# Build a pipeline with a placeholder classifier (LogisticRegression)\n",
    "clf_lr = Pipeline(steps=[('preprocessor', preprocessor), ('clf', LogisticRegression(max_iter=1000))])\n",
    "\n",
    "# Quick train-test split and fit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print('Training LogisticRegression pipeline...')\n",
    "clf_lr.fit(X_train, y_train)\n",
    "print('Done. Sample predictions:')\n",
    "print(clf_lr.predict(X_test)[:10])\n",
    "\n",
    "# Evaluate basic metrics\n",
    "y_pred = clf_lr.predict(X_test)\n",
    "print('\\\\nLogistic Regression basic evaluation:')\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall   :', recall_score(y_test, y_pred))\n",
    "print('F1       :', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bddca1",
   "metadata": {},
   "source": [
    "## Model Selection: Logistic Regression, Random Forest, Gradient Boosting, XGBoost\n",
    "\n",
    "Justification:\n",
    "- Logistic Regression: interpretable baseline.\n",
    "- Random Forest: robust, handles non-linearities and interactions.\n",
    "- Gradient Boosting (sklearn): strong predictive performance.\n",
    "- XGBoost: powerful gradient boosting implementation (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee65f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Train multiple models and compare metrics\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "}\n",
    "\n",
    "# Try to add XGBoost if available\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    models['XGBoost'] = XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "except Exception as e:\n",
    "    print('XGBoost not available or import failed:', e)\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), ('clf', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    res = {\n",
    "        'model': name,\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, pipe.predict_proba(X_test)[:,1]) if hasattr(pipe, 'predict_proba') else float('nan')\n",
    "    }\n",
    "    results.append(res)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c17984",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (GridSearchCV)\n",
    "\n",
    "We perform GridSearchCV on the entire pipeline (including preprocessor). We'll tune RandomForest hyperparameters as an example. Use `f1_weighted` or `roc_auc` as scoring to handle imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: GridSearchCV on pipeline (RandomForest)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_rf = Pipeline([('preprocessor', preprocessor), ('clf', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 200],\n",
    "    'clf__max_depth': [None, 10, 20],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe_rf, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=1)\n",
    "print('Running GridSearchCV (this may take some time)...')\n",
    "grid.fit(X_train, y_train)\n",
    "print('Best params:', grid.best_params_)\n",
    "print('Best score (f1_weighted):', grid.best_score_)\n",
    "\n",
    "best_rf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98dc765",
   "metadata": {},
   "source": [
    "## Handling Class Imbalance\n",
    "\n",
    "We add a resampling step (SMOTE) inside the pipeline using `imblearn`'s `SMOTE`. If imblearn is not installed, we fallback to class_weight='balanced' in classifiers.\n",
    "\n",
    "We'll compare performance with and without SMOTE/class_weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0058825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Compare imbalance strategies\n",
    "use_smote = False\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    use_smote = True\n",
    "    print('imblearn available: SMOTE will be used in resampling pipeline')\n",
    "except Exception as e:\n",
    "    print('imblearn not available:', e)\n",
    "\n",
    "if use_smote:\n",
    "    # Build imbalanced pipeline with SMOTE\n",
    "    imb_pipe = ImbPipeline(steps=[('preprocessor', preprocessor), ('smote', SMOTE(random_state=42)), ('clf', RandomForestClassifier(n_estimators=200, random_state=42))])\n",
    "    imb_pipe.fit(X_train, y_train)\n",
    "    y_pred_smote = imb_pipe.predict(X_test)\n",
    "    print('\\\\nWith SMOTE - RandomForest metrics:')\n",
    "    print('Precision:', precision_score(y_test, y_pred_smote))\n",
    "    print('Recall   :', recall_score(y_test, y_pred_smote))\n",
    "    print('F1       :', f1_score(y_test, y_pred_smote))\n",
    "else:\n",
    "    # Fallback: use class_weight='balanced' in classifier\n",
    "    pipe_cw = Pipeline([('preprocessor', preprocessor), ('clf', RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42))])\n",
    "    pipe_cw.fit(X_train, y_train)\n",
    "    y_pred_cw = pipe_cw.predict(X_test)\n",
    "    print('\\\\nWith class_weight=balanced - RandomForest metrics:')\n",
    "    print('Precision:', precision_score(y_test, y_pred_cw))\n",
    "    print('Recall   :', recall_score(y_test, y_pred_cw))\n",
    "    print('F1       :', f1_score(y_test, y_pred_cw))\n",
    "\n",
    "# Also print baseline (no imbalance handling) for comparison\n",
    "pipe_nom = Pipeline([('preprocessor', preprocessor), ('clf', RandomForestClassifier(n_estimators=200, random_state=42))])\n",
    "pipe_nom.fit(X_train, y_train)\n",
    "y_pred_nom = pipe_nom.predict(X_test)\n",
    "print('\\\\nNo imbalance handling - RandomForest metrics:')\n",
    "print('Precision:', precision_score(y_test, y_pred_nom))\n",
    "print('Recall   :', recall_score(y_test, y_pred_nom))\n",
    "print('F1       :', f1_score(y_test, y_pred_nom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Final evaluation\n",
    "# Choose final_model in order of preference: best_rf (from grid) -> imb_pipe (SMOTE) -> pipe_cw -> pipe_nom\n",
    "final_model = None\n",
    "try:\n",
    "    final_model = best_rf\n",
    "    print('Using best_rf from GridSearchCV as final model.')\n",
    "except Exception:\n",
    "    if 'use_smote' in globals() and use_smote:\n",
    "        final_model = imb_pipe\n",
    "        print('Using SMOTE pipeline as final model.')\n",
    "    else:\n",
    "        final_model = pipe_cw\n",
    "        print('Using class_weight-balanced pipeline as final model.')\n",
    "\n",
    "# Evaluate\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "if hasattr(final_model, 'predict_proba'):\n",
    "    y_proba = final_model.predict_proba(X_test)[:,1]\n",
    "else:\n",
    "    y_proba = None\n",
    "\n",
    "print('\\\\nFinal model metrics on test set:')\n",
    "print('Precision:', precision_score(y_test, y_pred_final))\n",
    "print('Recall   :', recall_score(y_test, y_pred_final))\n",
    "print('F1       :', f1_score(y_test, y_pred_final))\n",
    "if y_proba is not None:\n",
    "    print('ROC AUC  :', roc_auc_score(y_test, y_proba))\n",
    "\n",
    "print('\\\\nClassification Report:\\\\n')\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print('\\\\nConfusion Matrix:\\\\n', cm)\n",
    "\n",
    "# Visualize top 10 feature importances if tree-based\n",
    "try:\n",
    "    clf_step = final_model.named_steps['clf'] if isinstance(final_model, Pipeline) else final_model\n",
    "    importances = None\n",
    "    if hasattr(clf_step, 'feature_importances_'):\n",
    "        importances = clf_step.feature_importances_\n",
    "    if importances is not None:\n",
    "        # Attempt to reconstruct feature names\n",
    "        def get_feature_names_from_column_transformer(ct, input_features):\n",
    "            output_features = []\n",
    "            for name, trans, cols in ct.transformers:\n",
    "                if name == 'remainder':\n",
    "                    continue\n",
    "                if hasattr(trans, 'named_steps') and 'onehot' in trans.named_steps:\n",
    "                    ohe = trans.named_steps['onehot']\n",
    "                    try:\n",
    "                        cats = ohe.categories_\n",
    "                        for i, c in enumerate(cols):\n",
    "                            for cat in cats[i]:\n",
    "                                output_features.append(f\\\"{c}__{cat}\\\")\n",
    "                    except Exception:\n",
    "                        output_features.extend(cols)\n",
    "                else:\n",
    "                    output_features.extend(cols)\n",
    "            return output_features\n",
    "\n",
    "        feature_names = get_feature_names_from_column_transformer(final_model.named_steps['preprocessor'], X.columns)\n",
    "        feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(10)\n",
    "        print('\\\\nTop 10 feature importances:')\n",
    "        display(feat_imp)\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.barh(feat_imp.index[::-1], feat_imp.values[::-1])\n",
    "        plt.title('Top 10 feature importances')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print('Could not extract feature importances:', e)\n",
    "\n",
    "# Business interpretation\n",
    "print('\\\\nBusiness interpretation of confusion matrix:')\n",
    "print('Rows = Actual [0=no churn, 1=churn], Columns = Predicted [0=no churn, 1=churn]')\n",
    "print('cm =', cm)\n",
    "print('\\\\n- False Positive (predict churn when customer stays): costs include unnecessary retention offers or resource allocation.')\n",
    "print('- False Negative (predict non-churn when customer actually churns): costs include lost revenue and missed retention opportunity.')\n",
    "print('\\\\nTypically, False Negatives are more costly for churn tasks because you fail to retain a customer who will leave. Therefore recall for the churn class is often prioritized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4026d",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Notes & Next Steps\n",
    "\n",
    "- The notebook is ready to run. Depending on package availability (xgboost, imblearn), certain sections may need those packages installed. If a package is missing, you can either install it (`pip install xgboost imbalanced-learn`) or skip the optional parts.\n",
    "- You can extend the hyperparameter grid, add cross-validation strategies, or calibrate probabilities for better business decisions.\n",
    "\n",
    "**Generated by ChatGPT — file saved to `/mnt/data/telecom_churn_analysis.ipynb`.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
